<!DOCTYPE html>
<html lang="en">
	<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="DLTA Solutions">

    <title>A distributed, secure and immutable ledger of records</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/landing-page.css">

    <!-- Custom Fonts -->
    <link rel="stylesheet" type="text/css" href="/font-awesome-4.1.0/css/font-awesome.min.css">
    <link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

	<body>
		<!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            
                <a class="navbar-brand page-scroll" href="http://localhost:4000">
            
            		DLTA Solutions
            	</a>
        </div>
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a class="page-scroll" href="#a-propos">À propos</a>
                </li>                     
                <li>
                    <a class="page-scroll" href="#aidchain">AidChain</a>
                </li>
                <li>
                    <a class="page-scroll" href="#equipe">Équipe</a>
                </li>                	    
                <li>
                    <a class="page-scroll" href="/blog">Blog</a>
                </li>		    
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

		<!-- Header -->

<div class="intro-header" style="background-color: black">

	<div class="container">
		<div class="intro-message">
			<h2 class="">A distributed, secure and immutable ledger of records</h2>
			<br>
			05 Dec 2017 &#8729; <span class="meta"><a href="#">Youssef de Madeen Amadou</a></span>
		</div>
	</div>
</div>

<!-- Content -->
<div class="content-section-b">
	<div class="container">
    	<div class="post">
    		<div style="display:flex; justify-content:center;">
				<div class="col-xs-12 col-md-8 col-sm-10">
					<p>Dans cet article, j’explique comment utiliser R (ou tout autre programme que Java) pour faire du traitement distribué de données sur Hadoop. Pour moi qui n’ai jamais appris Java, cette idée est simplement géniale. La façon la plus simple d’y arriver, c’est avec l’API Streaming, lequel interprète des instructions de type Map et Reduce (séparément, comme nous le verrons) transmises dans un langage autre que Java (par exemple Ruby, Shell, etc.) et se charge de les exécuter comme une tâche typique MapReduce.</p>

<p>Ensemble nous allons écrire un petit programme <strong>Wordcount</strong> (pour compter les occurrences de mots dans une phrase) et utiliser l’API Streaming pour traiter la tâche de façon distribuée.</p>

<p>Pour commencer, nous avons besoin d’installer R sur le serveur ou la machine hôte du cluster, puisque c’est de R qu’il s’agit. Pour ça, exécuter (en tant que super utilisateur <em>root</em>) :</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">yum</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="o">-</span><span class="n">y</span><span class="w"> </span><span class="n">R</span></code></pre></figure>

<h3 id="lapi-streaming">L’API Streaming</h3>

<p>Comme je l’ai expliqué, Streaming se charge de traduire des instructions écrites dans un langage autre que Java, en une tâche typique MapReduce. L’API, disponible sur toutes les installations d’Hadoop, requiert un script au stade Map, un script au stade Reduce, un fichier de données à traiter, le nombre de fragmentations désirées pour la tâche, et un fichier pour stocker le rendu du traitement.</p>

<p>Dans le cas de notre exemple, avec <strong>Wordcount</strong>, voici à quoi ressemble la commande :</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar <span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/share/hadoop/tools/lib/hadoop-streaming<span class="k">*</span>.jar <span class="se">\</span>
<span class="nt">-input</span> /wc.txt <span class="se">\</span>
<span class="nt">-output</span> /wc-r.result <span class="se">\</span>
<span class="nt">-mapper</span> <span class="s1">'cat'</span> <span class="se">\</span>
<span class="nt">-reducer</span> /wc.r <span class="se">\</span>
<span class="nt">-numReduceTasks</span> 4 <span class="se">\</span>
<span class="nt">-file</span> wc.r <span class="o">&amp;&amp;</span> <span class="se">\</span>
hadoop fs <span class="nt">-cat</span> /wc-r.result/<span class="k">*</span></code></pre></figure>

<ul>
  <li><code class="highlighter-rouge">${HADOOP_PREFIX}/share/hadoop/tools/lib/hadoop-streaming*.jar</code> indique l’emplacement du jar de Streaming ; j’utilise * dans &lt;hadoop-streaming*.jar&gt; pour rester neutre vis-à-vis de la version d’Hadoop que vous pourriez utiliser</li>
  <li><code class="highlighter-rouge">input /wc.txt</code> : <strong>input</strong> spécifie l’emplacement du fichier de données (ici <em>wc.txt</em>) ; il faut noter que cet emplacement se réfère à HDFS, le système de stockage distribué d’Hadoop. Les données doivent donc être préalablement copiées sur HDFS</li>
  <li><code class="highlighter-rouge">output /wc-r.result</code> : <strong>output</strong> indique le fichier d’enregistrement du rendu de traitement (ici <em>wc-r.result</em>), toujours dans HDFS. Ce fichier sera créé lors de l’exécution de la tâche</li>
  <li><code class="highlighter-rouge">mapper 'cat'</code> : <strong>mapper</strong> spécifie le script du stade Map. Ici, la commande Shell <code class="highlighter-rouge">cat</code> (affiche le contenu de fichier texte), permet de faire passer nos données au stade Reduce sans modification</li>
  <li><code class="highlighter-rouge">reducer /wc.r</code> : <strong>reducer</strong> spécifie le script du stade Reduce. Ici, <em>wc.r</em>, indique le nom du fichier du script R pour wordcount. Nous verrons par la suite comment le créer.</li>
  <li><code class="highlighter-rouge">numReduceTasks 4</code> : indique 4 fragmentations au stade Reduce</li>
  <li><code class="highlighter-rouge">file wc.r</code> : l’argument <strong>file</strong> spécifie l’emplacement du fichier de script utilisé au stade Reduce. A supposer que nous ayons specifié un script R au stade Map, par exemple le fichier <em>wc_map.r</em>, il aurait fallu mettre <code class="highlighter-rouge">file wc.r wc_map.r</code>. Notez que ces emplacements se réfèrent au stockage du serveur ou la machine hôte, et non à HDFS.</li>
</ul>

<h3 id="les-données">Les données</h3>
<p>Nous créons notre fichier de données sur HDFS, en exécutant :</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">echo</span> <span class="s2">"This is an exemple of wordcount. This is an exemple of wordcount."</span> | hadoop fs <span class="nt">-appendToFile</span> - /wc.txt</code></pre></figure>

<p>Nous allons donc appliquer <strong>Wordcount</strong> sur le court paraphrage «This is an exemple of wordcount. This is an exemple of wordcount.».</p>

<h3 id="algorithme-r-pour-wordcount">Algorithme R pour Wordcount</h3>
<p>Notre script R ressemble à ceci :</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">#!/usr/bin/env Rscript</span><span class="w">
</span><span class="n">f</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">file</span><span class="p">(</span><span class="s2">"stdin"</span><span class="p">,</span><span class="n">open</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'r'</span><span class="p">)</span><span class="w">
</span><span class="n">tmp1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">readLines</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">warn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">F</span><span class="p">)</span><span class="w">
</span><span class="n">write.table</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">factor</span><span class="p">(</span><span class="n">unlist</span><span class="p">(</span><span class="n">strsplit</span><span class="p">(</span><span class="n">tmp1</span><span class="p">,</span><span class="s2">"\s|\s+"</span><span class="p">)))),</span><span class="n">quote</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="n">row.names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="n">col.names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"\t"</span><span class="p">)</span><span class="w">
</span><span class="n">close</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="w">
</span><span class="c1">#END</span></code></pre></figure>

<p>Nous avons besoin de créer le fichier <wc.r> avec `touch wc.r` ; puis l’éditer avec `nano wc.r`. Nous copions et collons simplement le code dans l’éditeur nano ; ici vous n'êtes pas tenu d'utiliser nano. Une fois le script collé, quitter en sauvegardant les modifications (avec nano, faire &lt;ctrl + x&gt; et confirmer les changements).</wc.r></p>

<h3 id="traitement-distribué">Traitement distribué</h3>
<p>Pour procéder au traitement des données, exécuter :</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar <span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/share/hadoop/tools/lib/hadoop-streaming<span class="k">*</span>.jar <span class="se">\</span>
<span class="nt">-input</span> /wc.txt <span class="se">\</span>
<span class="nt">-output</span> /wc-r.result <span class="se">\</span>
<span class="nt">-mapper</span> <span class="s1">'cat'</span> <span class="se">\</span>
<span class="nt">-reducer</span> /wc.r <span class="se">\</span>
<span class="nt">-numReduceTasks</span> 4 <span class="se">\</span>
<span class="nt">-file</span> wc.r</code></pre></figure>

<h3 id="résultats">Résultats</h3>
<p>Le rendu du traitement sera enregistré dans le fichier <wc-r.result> tel qu’indiqué, et nous pouvons le voir, en faisant :</wc-r.result></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop fs <span class="nt">-cat</span> /wc-r.result/<span class="k">*</span></code></pre></figure>

<p><img src="http://localhost:4000/assets/images/posts/Du-traitement-distribue-de-donnes-sur-Hadoop-en-utilisant-R/wordcount-r.PNG" alt="" /></p>

<h3 id="résultats-avec-java">Résultats avec Java</h3>

<p>Une façon de vérifier que le script R fait du bon travail, est de faire la même démarche en utilisant cette fois-ci le jar de Wordcount, fourni avec toutes les installations Apache Hadoop. Pour ce faire, exécuter ceci :</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">hadoop jar <span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/share/hadoop/mapreduce/hadoop-mapreduce-example<span class="k">*</span>.jar wordcount /wc.txt /wc-java.result <span class="o">&amp;&amp;</span> <span class="se">\</span>
hadoop fs <span class="nt">-cat</span> /wc-java.result/<span class="k">*</span></code></pre></figure>

<p><img src="http://localhost:4000/assets/images/posts/Du-traitement-distribue-de-donnes-sur-Hadoop-en-utilisant-R/wordcount-java.PNG" alt="" /></p>

<p>Voila ! Ce sera tout pour cet article. Ensemble, nous avons vu comment mettre en œuvre du traitement distribué de données sur Hadoop à partir de R :</p>

<ul>
  <li>Comment utiliser l’API Streaming d’Apache Hadoop</li>
  <li>Comment créer, éditer et lire un fichier texte sur HDFS</li>
  <li>Comment créer une fonction R pour compter les occurrences des mots d’une phrase (wordcount)</li>
</ul>

<p>Merci pour cette lecture!</p>

				</div>
			</div>
		</div>
	</div>
</div>
		<!-- Footer -->
<footer>
    <div class="social-buttons" style="text-align: center;">      
		<div>
			<ul class="list-inline">
				<li>
					<a href="https://github.com/dltasolutions">
						<span class="fa-stack fa-lg">
							<i class="fa fa-github fa-2x fa-fw"></i>
						</span>
					</a>
				</li>
				<li>
					<a href="">
						<span class="fa-stack fa-lg">
							<i class="fa fa-twitter fa-2x fa-fw"></i>
						</span>
					</a>
				</li>
				<li>
					<a href="">
						<span class="fa-stack fa-lg">
							<i class="fa fa-facebook fa-2x fa-fw"></i>
						</span>
					</a>
          		</li>
          		<li>
					<a href="">
						<span class="fa-stack fa-lg">
							<i class="fa fa-envelope-o fa-2x fa-fw"></i>
						</span>
					</a>
          		</li>
			</ul>
		</div>
	</div>
	<div class="copyright text-muted small" style="text-align: center;"> 
		<p>&copy; 2017 DLTA Solutions.</p>
		<!--<p>Site développé à partir du thème <a href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> par <a href="http://startbootstrap.com/">Start Bootstrap</a>.</p>-->
	</div>
</footer>
		<!-- jQuery Version 1.11.0 -->
<script src="/js/jquery-1.11.0.js"></script>

<!-- Plugin JavaScript -->
<script src="/js/jquery.easing.min.js" | prepend: site.baseurl"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js" | prepend: site.baseurl"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/landing-page.js"></script>
	</body>
</html>
